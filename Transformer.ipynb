{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"pgUfKaJfvHER","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":855},"outputId":"8a59d9a2-f433-4d21-aba6-fdac87a654a9","executionInfo":{"status":"ok","timestamp":1569856637303,"user_tz":-330,"elapsed":21265,"user":{"displayName":"Rajprakash Bale","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCVKR9vCOubo6YKttfZfaqFZOBauNRu-0NJKYEfww=s64","userId":"16747192638537868502"}}},"source":["!pip install pytorch-transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pytorch-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n","\r\u001b[K     |█▉                              | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 3.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n","Collecting sentencepiece (from pytorch-transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 41.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.224)\n","Collecting regex (from pytorch-transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n","\u001b[K     |████████████████████████████████| 655kB 37.6MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n","Collecting sacremoses (from pytorch-transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/04/b92425ca552116afdb7698fa3f00ca1c975cfd86a847cf132fd813c5d901/sacremoses-0.0.34.tar.gz (859kB)\n","\u001b[K     |████████████████████████████████| 860kB 38.1MB/s \n","\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.224)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.13.2)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch-transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch-transformers) (2.5.3)\n","Building wheels for collected packages: regex, sacremoses\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609238 sha256=66b8834e6e2410478f954b1718d14de5c96f2975257075c165a6339c5977c755\n","  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.34-cp36-none-any.whl size=883992 sha256=4a8a5c1ceb835fe6f3a44b71a068c5cc17d9b1d783ed632fc9374e75f0af7687\n","  Stored in directory: /root/.cache/pip/wheels/07/b9/5b/8bd674c23e962fbff34420a9fa7a2c374d591ecadd5bc37684\n","Successfully built regex sacremoses\n","Installing collected packages: sentencepiece, regex, sacremoses, pytorch-transformers\n","Successfully installed pytorch-transformers-1.2.0 regex-2019.8.19 sacremoses-0.0.34 sentencepiece-0.1.83\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IxgciXCNvQKj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"fa28465b-4247-407d-c5fa-d4f97337d136","executionInfo":{"status":"ok","timestamp":1569856832894,"user_tz":-330,"elapsed":58949,"user":{"displayName":"Rajprakash Bale","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCVKR9vCOubo6YKttfZfaqFZOBauNRu-0NJKYEfww=s64","userId":"16747192638537868502"}}},"source":["# Import required libraries\n","import torch\n","from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# Encode a text inputs\n","text = \"What is the fastest car in the\"\n","indexed_tokens = tokenizer.encode(text)\n","\n","# Convert indexed tokens in a PyTorch tensor\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","# Load pre-trained model (weights)\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# Set the model in evaluation mode to deactivate the DropOut modules\n","model.eval()\n","\n","# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","model.to('cuda')\n","\n","# Predict all tokens\n","with torch.no_grad():\n","    outputs = model(tokens_tensor)\n","    predictions = outputs[0]\n","    \n","    \n","# Get the predicted next sub-word\n","predicted_index = torch.argmax(predictions[0, -1, :]).item()\n","predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","\n","# Print the predicted word\n","print(predicted_text)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["100%|██████████| 1042301/1042301 [00:00<00:00, 5871589.58B/s]\n","100%|██████████| 456318/456318 [00:00<00:00, 1718663.58B/s]\n","100%|██████████| 176/176 [00:00<00:00, 54355.17B/s]\n","100%|██████████| 548118077/548118077 [00:34<00:00, 15786517.62B/s]\n"],"name":"stderr"},{"output_type":"stream","text":[" What is the fastest car in the world\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uM-uu7cEwV7U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"764a8b59-69ba-4b74-d079-91187bf11881","executionInfo":{"status":"ok","timestamp":1569857668386,"user_tz":-330,"elapsed":1474,"user":{"displayName":"Rajprakash Bale","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCVKR9vCOubo6YKttfZfaqFZOBauNRu-0NJKYEfww=s64","userId":"16747192638537868502"}}},"source":["# Encode a text inputs\n","text = \"i am not going to come to the\"\n","indexed_tokens = tokenizer.encode(text)\n","\n","# Convert indexed tokens in a PyTorch tensor\n","tokens_tensor = torch.tensor([indexed_tokens])\n","\n","# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","model.to('cuda')\n","\n","# Predict all tokens\n","with torch.no_grad():\n","    outputs = model(tokens_tensor)\n","    predictions = outputs[0]\n","    \n","    \n","# Get the predicted next sub-word\n","predicted_index = torch.argmax(predictions[0, -1, :]).item()\n","predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","\n","# Print the predicted word\n","print(predicted_text)\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":[" i am not going to come to the conclusion\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z-RSxn1RxVOy","colab_type":"text"},"source":["# Generating the next sentence"]},{"cell_type":"code","metadata":{"id":"RVzuCfNpw7F9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":148},"outputId":"663ff099-2adf-4114-951c-9288c86e2878","executionInfo":{"status":"ok","timestamp":1569857129737,"user_tz":-330,"elapsed":6317,"user":{"displayName":"Rajprakash Bale","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCVKR9vCOubo6YKttfZfaqFZOBauNRu-0NJKYEfww=s64","userId":"16747192638537868502"}}},"source":["!git clone https://github.com/huggingface/pytorch-transformers.git"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Cloning into 'pytorch-transformers'...\n","remote: Enumerating objects: 10, done.\u001b[K\n","remote: Counting objects:  10% (1/10)\u001b[K\rremote: Counting objects:  20% (2/10)\u001b[K\rremote: Counting objects:  30% (3/10)\u001b[K\rremote: Counting objects:  40% (4/10)\u001b[K\rremote: Counting objects:  50% (5/10)\u001b[K\rremote: Counting objects:  60% (6/10)\u001b[K\rremote: Counting objects:  70% (7/10)\u001b[K\rremote: Counting objects:  80% (8/10)\u001b[K\rremote: Counting objects:  90% (9/10)\u001b[K\rremote: Counting objects: 100% (10/10)\u001b[K\rremote: Counting objects: 100% (10/10), done.\u001b[K\n","remote: Compressing objects:  10% (1/10)\u001b[K\rremote: Compressing objects:  20% (2/10)\u001b[K\rremote: Compressing objects:  30% (3/10)\u001b[K\rremote: Compressing objects:  40% (4/10)\u001b[K\rremote: Compressing objects:  50% (5/10)\u001b[K\rremote: Compressing objects:  60% (6/10)\u001b[K\rremote: Compressing objects:  70% (7/10)\u001b[K\rremote: Compressing objects:  80% (8/10)\u001b[K\rremote: Compressing objects:  90% (9/10)\u001b[K\rremote: Compressing objects: 100% (10/10)\u001b[K\rremote: Compressing objects: 100% (10/10), done.\u001b[K\n","Receiving objects:   0% (1/9227)   \rReceiving objects:   1% (93/9227)   \rReceiving objects:   2% (185/9227)   \rReceiving objects:   3% (277/9227)   \rReceiving objects:   4% (370/9227)   \rReceiving objects:   5% (462/9227)   \rReceiving objects:   6% (554/9227)   \rReceiving objects:   7% (646/9227)   \rReceiving objects:   8% (739/9227)   \rReceiving objects:   9% (831/9227)   \rReceiving objects:  10% (923/9227)   \rReceiving objects:  11% (1015/9227)   \rReceiving objects:  12% (1108/9227)   \rReceiving objects:  13% (1200/9227)   \rReceiving objects:  14% (1292/9227)   \rReceiving objects:  15% (1385/9227)   \rReceiving objects:  16% (1477/9227)   \rReceiving objects:  17% (1569/9227)   \rReceiving objects:  18% (1661/9227)   \rReceiving objects:  19% (1754/9227)   \rReceiving objects:  20% (1846/9227)   \rReceiving objects:  21% (1938/9227)   \rReceiving objects:  22% (2030/9227)   \rReceiving objects:  23% (2123/9227)   \rReceiving objects:  24% (2215/9227)   \rReceiving objects:  25% (2307/9227)   \rReceiving objects:  26% (2400/9227)   \rReceiving objects:  27% (2492/9227)   \rReceiving objects:  28% (2584/9227)   \rReceiving objects:  29% (2676/9227)   \rReceiving objects:  30% (2769/9227)   \rReceiving objects:  31% (2861/9227)   \rReceiving objects:  32% (2953/9227)   \rReceiving objects:  33% (3045/9227)   \rReceiving objects:  34% (3138/9227)   \rReceiving objects:  35% (3230/9227)   \rReceiving objects:  36% (3322/9227)   \rReceiving objects:  37% (3414/9227)   \rReceiving objects:  38% (3507/9227)   \rReceiving objects:  39% (3599/9227)   \rReceiving objects:  40% (3691/9227)   \rReceiving objects:  41% (3784/9227)   \rReceiving objects:  42% (3876/9227)   \rReceiving objects:  43% (3968/9227)   \rReceiving objects:  44% (4060/9227)   \rReceiving objects:  45% (4153/9227)   \rReceiving objects:  46% (4245/9227)   \rReceiving objects:  47% (4337/9227)   \rReceiving objects:  48% (4429/9227)   \rReceiving objects:  49% (4522/9227)   \rReceiving objects:  50% (4614/9227)   \rReceiving objects:  51% (4706/9227)   \rReceiving objects:  52% (4799/9227)   \rReceiving objects:  53% (4891/9227)   \rReceiving objects:  54% (4983/9227)   \rReceiving objects:  55% (5075/9227)   \rReceiving objects:  56% (5168/9227)   \rReceiving objects:  57% (5260/9227)   \rReceiving objects:  58% (5352/9227)   \rReceiving objects:  59% (5444/9227)   \rReceiving objects:  60% (5537/9227)   \rReceiving objects:  61% (5629/9227)   \rReceiving objects:  62% (5721/9227)   \rReceiving objects:  63% (5814/9227)   \rReceiving objects:  64% (5906/9227)   \rReceiving objects:  65% (5998/9227)   \rReceiving objects:  66% (6090/9227)   \rReceiving objects:  67% (6183/9227)   \rReceiving objects:  68% (6275/9227)   \rReceiving objects:  69% (6367/9227)   \rReceiving objects:  70% (6459/9227)   \rReceiving objects:  71% (6552/9227)   \rReceiving objects:  72% (6644/9227)   \rReceiving objects:  73% (6736/9227)   \rReceiving objects:  74% (6828/9227)   \rReceiving objects:  75% (6921/9227)   \rReceiving objects:  76% (7013/9227)   \rReceiving objects:  77% (7105/9227)   \rReceiving objects:  78% (7198/9227)   \rReceiving objects:  79% (7290/9227)   \rReceiving objects:  80% (7382/9227)   \rReceiving objects:  81% (7474/9227)   \rReceiving objects:  82% (7567/9227)   \rReceiving objects:  83% (7659/9227)   \rReceiving objects:  84% (7751/9227)   \rReceiving objects:  85% (7843/9227)   \rReceiving objects:  86% (7936/9227)   \rReceiving objects:  87% (8028/9227)   \rReceiving objects:  88% (8120/9227)   \rReceiving objects:  89% (8213/9227)   \rReceiving objects:  90% (8305/9227)   \rReceiving objects:  91% (8397/9227)   \rReceiving objects:  92% (8489/9227)   \rReceiving objects:  93% (8582/9227)   \rReceiving objects:  94% (8674/9227)   \rReceiving objects:  95% (8766/9227)   \rReceiving objects:  96% (8858/9227)   \rReceiving objects:  97% (8951/9227)   \rReceiving objects:  98% (9043/9227)   \rremote: Total 9227 (delta 1), reused 2 (delta 0), pack-reused 9217\u001b[K\n","Receiving objects: 100% (9227/9227), 4.87 MiB | 19.93 MiB/s, done.\n","Resolving deltas: 100% (6694/6694), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5ZseR1Q2yOUH","colab_type":"code","colab":{}},"source":["!cp -avr pytorch-transformers/transformers pytorch-transformers/examples/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fp1S49BKxrQT","colab_type":"code","colab":{}},"source":["\n","!python pytorch-transformers/examples/run_generation.py \\\n","    --model_type=gpt2 \\\n","    --length=100 \\\n","    --model_name_or_path=gpt2 \\"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8DBRPMmx5QY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}